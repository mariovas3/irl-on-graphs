\documentclass{report}
\usepackage{setspace}
\usepackage{bookmark}
%\usepackage{subfigure}

% my extra package imports;
\usepackage{caption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{float}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{tikz}
\usetikzlibrary {positioning, shapes, bayesnet}
\graphicspath{{./}}
\usepackage{natbib}
% end of my imports;

\pagestyle{plain}
\usepackage{amssymb,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
% \newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%

% COVER PAGE;
\title{
  { \includegraphics[scale=.5]{ucl_logo.png}}\\
  {{\Huge Graph style transfer via Inverse Reinforcement Learning}}\\
  {\large An application to subway networks}\\
}

\date{Submission date: Day Month Year}
\author{
  Candidate Number: YLLK3\thanks{
      {\bf Disclaimer:}
      This report is submitted as part requirement 
      for the MSc Machine Learning at UCL. It is
      substantially the result of my own work except 
      where explicitly indicated in the text.
      The report may be freely copied and 
      distributed provided the source is explicitly acknowledged.
    }
    \\ \\
  MSc Machine Learning\\ \\
  Supervisor's name
}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}  
\numberwithin{algorithm}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START DOCUMENT;
\begin{document}
\onehalfspacing
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT;
\begin{abstract}
  Summarise your report concisely.
\end{abstract}
\tableofcontents
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - INTRODUCTION;
\chapter{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - RL BACKGROUND;
\chapter{Reinforcement Learning Background}
\label{chap:RL}
\textbf{TODO}: A SHORT SUMMARY OF THE FOLLOWING SENTENCES IS TO BE PUT 
IN THE END OF THE INTRO WHERE I EXPLAIN THE STRUCTURE OF THE 
REPORT.

In this section I cover the necessary background to follow the 
rest of the work. The main algorithm is model-free but has value 
functions and an explicit parameterised policy. I will not cover 
model-based RL in detail in what is to follow, therefore. The 
following will summarise the knowledge about the goal of RL, 
MDPs, Value functions, Policy gradients, off-policy learning 
and Actor-Critics. The last is particularly relevant, given 
that the policy learning algorithm in the inner loop of the IRL 
problem can be thought of as a ``soft'' variant of an Actor-Critic 
learning algorithm.

% SECTION - THE GOAL OF RL;
\section{The Goal of Reinforcement Learning}
\label{sec:RLGoal}

The goal of Reinforcement learning is to learn a behaviour that 
optimises some reward signal. The key assumption is that 
any goal (be it high-level or otherwise) can be fomulated as 
maximisation of the reward signal.

In the Reinforcement learning setup, we distinguish between two 
key entities - environment and agent. The environment is the 
entity that specifies a set of rules based on which, an action 
is judged given the environment's configuration. 
More specifically, let the configuration or state of the 
environment at time $t$ be denoted by $s_t$. Then, 
given an action, $a_t$ at time $t$, the environment evaluates 
how good this action is, given its 
current state, and outputs a (scalar) reward signal (or reward), 
$r_{t+1}$, and also updates its state to $s_{t+1}$. The agent 
is the entity responsible for giving an action $a_t$. 

It is worth noting that there are 
different conventions about indexing the reward, but in this work 
we adopt the convention from the Sutton and Barto book 
\citep{Sutton1998} where the reward is thought of as feedback 
given to the agent the step after the action was taken.

The agent may not always have access 
to the environment's state and can maintain its own state, based 
on which it takes an action. We refer to such settings as partially 
observed problems. 

An example of such a problem could be a robot 
whose goal is to find its way out of a maze as quickly as possible.
Suppose the robot only ``sees'' what is 
immediately surrounding it, in which case this would be its state 
or observation, $o_t$. 
In contrast, if it had access to the environment's state, it could 
see the shortest path out of the maze and take the necessary actions 
that yield the greatest accumulated reward (or return).

% SECTION - MDPs
\section{Markov Decision Processes (MDP)}
\label{sec:RLMDP}
Suppose the robot from the previous example has access to the 
environment state (sees the whole map). Then, given the 
current state of the environment 
and the action taken 
at that state, the next environment state does not depend on any 
past states and/or actions. That is, the 
environment state is first-order Markovian 
(given the present state and action). Indeed, this 
leads us to a convenint 
mathematical formulation of the dynamics of the 
Reinforcement learning problem called 
a Markov Decision Process (MDP).

An MDP with fully observed, Markovian states, is defined by the tuple 
($\mathcal{S},\; \mathcal{A},\; \mathcal{P},\; r(\cdot),\; \gamma$), where
\begin{itemize}
  \item $\mathcal{S}$ is the state space of the environment 
    (Markovian states).
  \item $\mathcal{A}$ is the action space (actions among which 
    the agent can select).
  \item $\mathcal{P}:\mathcal{S}\times \mathcal{A}\times \mathcal{S}\rightarrow [0, \infty)$ 
    is the transition operator and maps 
    the current state-action pair and the next state to the 
    correspondin probability density (in discrete state spaces, 
    the range will be $[0, 1]$).
  \item $r(\cdot)$ is the scalar-valued reward function 
    (usually defined as $r:\mathcal{S}\times \mathcal{A}\rightarrow \reals$, 
    although $r:\mathcal{S}\rightarrow \reals$ is also possible).
  \item $\gamma\in [0, 1]$ is the discount factor of rewards, which controls 
    how long or short sighted our objective is.
\end{itemize} 

The way $\gamma$ is used is typically alongside some function of 
the rewards collected along a path/trajectory. Let $\tau$ denote a 
path/trajectory given by:
\begin{equation}
  \tau:=(s_1, a_1, r_2, s_2, a_2, \ldots, s_T, a_T, r_{T+1}, s_{T+1}),\label{eq:tau} 
\end{equation} 
and $G:\mathcal{T}\rightarrow \reals$ denote the return 
function. Among the simplest examples of $G$ is the Monte-Carlo 
return:
\begin{equation}
  G(\tau):=\sum_{k=0}^{T-t}\gamma^kr_{t+k+1}.\label{eq:MCReturn}
\end{equation}
Looking at \eqref{eq:MCReturn}, we see that if we set $\gamma=0$ 
this corresponds to a myopic return, $G_t=r_{t+1}$, while if 
$\gamma=1$, we get a far-sighted return where we equally weigh 
all rewards along the trajectory $\tau$.

The MDP, as currently defined, induces a probabilistic graphical model 
(pgm) that is illustrated in Figure \ref{fig:MDP}. Due to D-separation 
we see that given $s_t$ and  $a_t$, $s_{t+1}$ is independent of 
$s_j$ and $a_j$ for all $j<t$.
Given a \textit{finite trajectory}/episode of length $T$, the induced 
factorisation is:
\begin{equation}
  p^{\pi}(\tau):=p(s_1)\prod_{t=1}^T \pi(a_t|s_t)p(s_{t+1}|s_t,a_t),
\end{equation}
where $\pi(\cdot)$ is known as the policy according to which the 
agent makes decisions and $p(s_{t+1}|s_t, a_t)$ is defined by the 
transition operator of the MDP, $\mathcal{P}$. Note the superscript 
in $p^{\pi}(\tau)$. This is there because we are giving the probability 
of the trajectory under the current behaviour of the agent, given 
by the policy $\pi$.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,
      on grid , inner sep=0pt, minimum size=7mm,
      semithick, state/.style={circle, draw}]
      % s1, a1, s2;
      \node[state](s_1) at(0,0) {$s_1$};
      \node[state](a_1) at(1,1) {$a_1$};                                                                                                       
      \path (s_1) edge (a_1);
      \node[state](s_2) at(3,0) {$s_2$};
      \path (s_1) edge (s_2);
      \path (a_1) edge (s_2);
      % s2, a2, s3
      \node[state] (a_2) at(4,1) {$a_2$};
      \path (s_2) edge (a_2);
      \node[state] (s_3) at(6,0){$s_3$};
      \path (s_2) edge (s_3);
      \path (a_2) edge (s_3);
      % dots;
      \node (dots) at(7.5,0) {$\ldots$};
      \path (s_3) edge (dots);
  \end{tikzpicture}
  \caption{\label{fig:MDP} Graph of MDP with Markovian states.}
\end{figure}

As mentioned earlier with the robot example, we can have our agent 
not observe the Markovian state, and instead receive an observation, 
$o_t$, from its sensors. This induces a Partially Observed MDP (POMDP) 
whose graphical model is presented in Figure \ref{fig:POMDP}. As we 
can read off of the pgm in figure \ref{fig:POMDP}, we see that given 
$s_t$ and $a_t$, $s_{t+1}$ is still independent of 
$o_{1:t}$, $s_{1:t-1}$ and $a_{1:t-1}$ by D-separation. However,
The observations are not Markovian in general since $o_j$ for $j<t$ 
can influence $o_{t+1}$ even if $o_t$ and $a_t$ are known (again by 
D-separation). The observation is, however, independent of the past
givent the current state (unobserved in the POMDP setting).

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,
      on grid , inner sep=0pt, minimum size=7mm,
      semithick, state/.style={circle, draw}]
      % s1, o1, a1, s2;
      \node[state](s_1) at(0,0) {$s_1$};
      \node[state](o_1) at (0, 1){$o_1$};
      \path (s_1) edge (o_1);
      \node[state](a_1) at(1.5,1) {$a_1$};
      \path (o_1) edge (a_1);
      \node[state](s_2) at(3,0) {$s_2$};
      \path (s_1) edge (s_2);
      \path (a_1) edge (s_2);
      % s2, o2, a2, s3
      \node[state](o_2) at (3,1) {$o_2$};
      \path (s_2) edge (o_2);
      \node[state] (a_2) at(4.5,1) {$a_2$};
      \path (o_2) edge (a_2);
      \node[state] (s_3) at(6,0){$s_3$};
      \path (s_2) edge (s_3);
      \path (a_2) edge (s_3);
      % dots;
      \node (dots) at(7.5,0) {$\ldots$};
      \path (s_3) edge (dots);
  \end{tikzpicture}
  \caption{\label{fig:POMDP} Graph of POMDP, $s_t$ is the Markovian 
  state, $o_t$ is the observation of the agent.}
\end{figure}

The associated factorisation based on the pgm in 
figure \ref{fig:POMDP} is:
\begin{equation}
  p^{\pi}(\tau)=p(s_1)\prod_{t=1}^T p(o_t|s_t)\pi(a_t|o_t)p(s_{t+1}|s_t, a_t).
\end{equation}
We see that in the POMDP setting, the agent makes decisions based 
on the observation and not the Markovian state. Depending on how 
informative the observation is, it may be difficult to construct 
a well-performing policy. Additionally, due to the non-Markovian 
nature of the observations, it is common to track and process old 
observations in order to form the agent state. Intuitively, one 
would expect the influence of observations from a long time ago 
to decay, so it might be useful to have a moving window over the 
collected experience.

In cases where we have deterministic transitions from state and action 
to the next state (and similarly from state to observation), 
$p(s_{t+1}|s_t, a_t)=\delta(s_{t+1},f(s_t, a_t))$, where 
$\delta(\cdot)$ denotes the delta distribution (placing all 
probability mass on $f(s_t, a_t)$) and 
$f: \mathcal{S}\times\mathcal{A}\rightarrow \mathcal{S}$ is some deterministic 
transition function (similarly $g:\mathcal{S}\rightarrow \mathcal{O}$ 
can be the sensor giving the observation).

% SECTION - FORMULATING EPISODIC AND CONTINUAL SETTING REWARD;
\section{Formulating objective functions}
\label{sec:RLObjectives}
\begin{itemize}
  \item Talk about episodic and continual settings;
  \item Talk about value functions;
\end{itemize}

% SECTION - improving the policy;
\section{title}



\begin{itemize}
  \item Define the RL objective - optimising long term reward 
    or the episodic objective with the returns;
  \item Give the Bellman equations as a means to solve an MDP;
    Give proof of the convergence of those Bellman operators;
  \item Make the claim about the existence of an optimal greedy 
    policy, given an MDP;
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTHER - IRL THEORY;
\chapter{Inverse Reinforcement Learning}
Test citation \citep{NgIRL} - done.
\begin{itemize}
  \item State the goal of IRL - estimate an objective function
    that when optimised should yield a behaviour similar to the 
    one in the demos by the expert.
  \item Argue the point about the ill-posedness of IRL, which makes 
    classic optimal control not suitable for the job.
  \item Talk about the max-margin approach (Ng) to IRL.
    State drawbacks;
  \item Talk about the max-entropy Ziebart approach;
    Show the (soft) ``duality'' between this probabilistic setting and 
    the updates from the Bellman equations - see Levine lectures 
    and motivations and his review paper. Here should talk about the 
    works of Todorov, Sergey's review paper and SAC as a means to 
    work in this probabilistic setting.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - GRAPH REPRESENTATIONS;
\chapter{Graph representations}
\begin{itemize}
  \item Give a lit review with the Scarsalli (can be enough to just 
  mention his name and give the general MPNN structure).
  \item Talk about the applications of GNNs - e.g., the Kipf paper 
    with GCN and application to citation graphs;
  \item Talk about the GATv2 which I am using.
  \item Read/skim the new paper about how much can gnns learn.
  \item State drawbacks and potential reasons, why it is difficult 
    to work with graph topography rather than topology when using 
    GNNs.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - OUR CONTRIBUTION;
\chapter{Our Contribution}
\begin{itemize}
  \item Basically the different spins on GrphOpt;
    \subitem Check effect of adding separate reward encoder;
    \subitem Check effect of UT trick - didn't seem so good, might drop;
    \subitem Check effect of multitask training - see if works;
    \subitem Talk about particular implementation experience
      gathering and storing it in the Replay Buffer; talk about 
      the batch-sampling time reward computation and not wasting 
      state-action trajectory experience by having to clear the 
      buffer after reward update, had we stayed with vanilla description;
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - EXPERIMENTS;
\chapter{Experiments}

\section{Description of tests}
\label{sec:test_description}
The experimental suite consists of four tests.
\begin{algorithm}
  \caption{Reward function transfer vs direct policy deployment}
  \label{tests:test_reward_transfer}
  \begin{algorithmic}
    \State $irlreward,\; irlpolicy \gets irlalgorithm(sourcegraph)$
    \State $irlstats,\; newpolicystats \gets [],\; []$
    \For{$t=1\ldots k$}
    \State $irlstats_t \gets collect\_irlstats(targetgraph)$
    \State $newpolicy_t \gets train\_policy(targetgraph,\; irlreward)$
    \State $newpolicystats_t \gets collect\_newpolicystats(targetgraph)$
    \State $irlstats.append(irlstats_t)$
    \State $newpolicystats.append(newpolicystats_t)$
    \EndFor
    \State $return\; (irlstats,\; newpolicystats)$
  \end{algorithmic}
\end{algorithm}
In Algorithm \ref{tests:test_reward_transfer}, by ``stats'', it is meant 
graph topology statistics such as triangles, clustering coefficients, 
degrees and rewards along generated paths associated with the 
relevant policy. The \verb|collect_stats| function deploys the relevant 
policy on the vertex set of the relevant graph, having emptied the 
edge set. The termination condition is when we have build the same 
number of edges as the original graph (passed to the \verb|collect_paths|
function) or when along the process of graph building we propose 
too many repeated edges (edges we had added earlier) or 
self-loops (connects a node to itself).

After these numbers are saved, we calculate summary 
statistics such as \verb|min|, \verb|median|, \verb|max|, 
\verb|mean|, \verb|standard_deviation| and 
then report the average and the standard deviations of the five number 
summaries ($k$ total) over the $k$ runs. For example, for each of the 
$k$ passes of the for loop of algorithm \ref{tests:test_reward_transfer} 
we get the degrees of all nodes, then take the \verb|median| degree for 
that pass and append it to the medians of the other passes. This way 
we obtain the median degree for each of the $k$ passes. We then average 
the $k$ numbers and calculate the standard deviation. The final reported 
value will then be the average median degree over $k$ passes, together 
with its standard deviation.

Based on the raw graph topology statistics (e.g., we concatenate the 
degrees of the nodes over the $k$ passes), we report the 
p-values from a two-sample Kolmogorov-Smirnov (KS) test against the 
graph topology statistics of the true graphs (source or target graph). 
This way we usually compare the distribution of a sample of $k\times |\mathcal{V}|$ 
numbers versus the distribution of $|\mathcal{V}|$ numbers (the latter being the 
true graph statistics), where $|\mathcal{V}|$ denotes the number 
of nodes/vertices in the relevant graph.

This test is used to determine whether we were able to learn a better 
graph construction policy for the target graph 
by retraining a new policy using the IRL-reward 
or by directly deploying the IRL-policy on the target graph.

Since the irlpolicy was trained on the source graph, we should expect 
it to construct a graph on the targetgraph vertex set, that resembles 
the topology of the source graph. On the other hand, if the source 
graph and the target graph are very similar, we should expect 
the learned reward from IRL (on the source graph) to give a useful 
criterion to guide the 
newpolicy in constructing graphs with similar topology to that 
of the target graph.

\begin{algorithm}
  \caption{Learning the source graph distribution}
  \label{tests:test_learn_source_graph_dist}
  \begin{algorithmic}
    \State $irlreward,\; irlpolicy \gets irlalgorithm(sourcegraph)$
    \State $irlstats \gets []$
    \For{$t=1\ldots k$}
    \State $irlstats_t\gets collect\_irlstats(sourcegraph)$
    \State $irlstats.append(irlstats_t)$
    \EndFor
    \State $return\; irlstats$
  \end{algorithmic}
\end{algorithm}
Similarly to Algorithm \ref{tests:test_reward_transfer}, ``stats'' 
is used for the graph topology statistics described above. We again 
calculate averages and standard deviations of the summary statistics 
over the $k$ runs, as well as p-values from the KS test.

This test is used to determine if the trained irlpolicy can be used 
to build graphs similar to the source graph (in terms of topology). 
As in the previous test, the \verb|collect_irlstats| function deploys 
the policy on the vertex set, having emptied the edge set.

Since the irlpolicy was trained on the source graph during IRL, 
we should expect it to be able to reconstruct graphs with similar 
topological properties. We judge this numerically by the p-value 
from two-sample KS tests.


\begin{enumerate}
  \item Run IRL training on a source graph. Receive trained policy 
  and reward function.
\end{enumerate}
\begin{itemize}
  \item T1 is about transferring learned reward on target graph;
  also compares learned policy directly on target, compared to 
  training new policy on target with frozen reward;
  \item T2 is about MRR - based on top 25 proposal edges;;
  \item T3 is about running learned policy on source graph and 
  seeing if we get similar graphs;
  \item I also get rewards along paths.
  \begin{itemize}
    \item Compare convergence rate of eval returns over seeds
      and comment on speed;
  \end{itemize}
  \item BA40 to BA80 - 10 seeds
  \begin{itemize}
    \item GOptEpWeights, GOptPerDec, 
      GOptRewEncEpWeights, GOptRewEncPerDec,
    \item THIS IS NOT APPLICABLE TO BA GRAPHS - ONLY TRY IT ON GRAPHS 
      WITH MEANINGFUL TOPOGRAPHY!!! - Then take better of EpWeights and PerDec and do 
      GOptMultitaskBetterWeight and GOptRewEncMultitaskBetterWeight,
    \item also run UT trick with the better weight setting;
  \end{itemize}
    
  \item T1 and T3 run together - this is $10\times 8=80$ experiments;
  \item T2 runs alone - this is $10\times 8=80$ experiments;
  \item total is 160 experiments;
  \item  Run best config on Urban nets 10 times;
\end{itemize}

\chapter{Conclusion and Further work}
This work builds on prior research in inverse reinforcement learning 
for graph domains. Our contributions are, firstly, the addition of the 
per-decision importance sampling for the IRL reward objective, 
secondly, the addition of a graph encoder to the reward function, 
and finally, an empirical comparison of applying the Unscented 
Transform \citep{JulierUT} versus the reparameterisation trick 
for approximating expectations.

We have empirically shown that the per-decision importance 
sampling increases
the stability of the algorithm over multiple seeds as seen by 
the plots of the returns during training.

Adding a graph encoder as part of the reward function, seems to 
have helped learn appropriate embeddings, which were useful for 
evaluating the states on the target graph. This is seen in the 
performance comparison between this method and 
the vanilla GraphOpt \citep{GraphOpt} 
implementation which uses a single graph encoder. Our method 
also allows for the ``full'' transfer of the reward function, 
rather than simply taking the MLP associated with the reward and 
then retraining a graph encoder and a policy. The latter actually 
can ``trick'' the reward to think a state is more advantageous than 
it actually is by providing an ``adversarial'' embedding that is 
mapped to a high reward from the reward MLP. Providing the reward 
with its own graph encoder, helped avoid such situations.

Finally, we showed that the UT trick also lead to more stable 
training of the algorithm as seen by the plots of the losses 
and the returns recorded during evaluation.

Despite the proposed improvements, it is still unclear what constitutes 
a good strategy for generating expert samples for the IRL objective. 
Merely giving a random permutation of the edge set removes the possible 
temporal component in building the graph. In many cases, such as 
road-network planning, previously built connections can actually 
be the main reason for new road segments. As a simple illustration 
of this idea, the reader is directed to Figure \ref{fig:bad_rand_perm}.
In contrast to most IRL scenarios where new demonstrations are generated, 
we only work with a single demonstration, which we assume the temporal 
component has no effect, allowing us to treat any permutation of the 
edge set as a separate expert trajectory. As seen in the figure, this 
could lead to very different demonstrations from the optimal control 
demonstration and may result in learning poor policies.

\begin{figure}[H]
  \centering
  \includegraphics[scale=0.5]{bad_rand_perm.png}
  \caption{\label{fig:bad_rand_perm} Optimal control path 
  in blue colour for both plots. The ``optimal control path'' 
  is simply a $sin(\cdot)$ function. The two x-marks are the 
  starting and ending point of all paths. It is assumed 
  we always start from the same position and the ``goal'' is also 
  at the same position. Left: different soft-optimal 
  paths sampled alongside the optimal control path (soft-optimal 
  paths generated by adding random Gaussian noise to the optimal 
  path). Right: different 
  random permutations of the optimal control path sampled alongside 
  the optimal control path.}
\end{figure}


% \chapter{title of first chapter}
% This is just a bare minimum to get started.  There is unlimited guidance on using latex, e.g. {\tt https://en.wikibooks.org/wiki/LaTeX}.   You are still responsible to check the detailed requirements of a project, including formatting instructions, see \\
% {\tt https://moodle.ucl.ac.uk/pluginfile.php/3591429/mod\_resource/content/7/UGProjects2017.pdf}.
% Leave at least a line of white space when you want to start a new paragraph.

% Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, \sum_{i=0}^nf(i)$, or in display mode
% \[ e^{i\pi}=-1\] and another way, this time with labels,
% \begin{align}
% \label{line1} A=B\wedge B=C&\rightarrow A=C\\
% &\rightarrow C=A\\
% \intertext{note that}
% n!&=\prod_{1\leq i\leq n}i \\
% \int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
% \end{align}
% We can refer to labels like this \eqref{line1}.   

% \chapter{title of second chapter}
% % Often lots of citations here (and elsewhere), e.g. \cite{Rey:D} or \cite[Theorem 2.3]{PriorNOP70}.   Bibtex can help with this, but is not essential. If you want pictures, try

% \begin{center}
% \includegraphics[scale=.5]{aristotle.jpg}
% \end{center}
% You can use 
% \begin{itemize}
% \item lists
% \item like this
% \end{itemize}
% or numbered
% \begin{enumerate}
% \item like this,
% \item or this
% \end{enumerate}
% but don't overdo it.
% \chapter{title of third chapter}
% If you have a formal theorem you might try this.
% \begin{definition}\label{def}
% See definition~\ref{def}.
% \end{definition}
% \begin{theorem}
% For all $n\in\nats,\; 1^n=1$.
% \end{theorem}
% \begin{proof}
% By induction over $n$.
% \end{proof}

% \chapter{etc.}
\appendix

\bibliographystyle{apa}
\bibliography{my_bib.bib}

\end{document}