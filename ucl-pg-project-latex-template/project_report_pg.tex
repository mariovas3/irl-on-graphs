\documentclass{report}
\usepackage{setspace}
\usepackage{bookmark}
%\usepackage{subfigure}

% my extra package imports;
\usepackage{caption}
\usepackage{float}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{tikz}
\usetikzlibrary {positioning, shapes, bayesnet}
\graphicspath{{./}}
\usepackage{natbib}
% end of my imports;

\pagestyle{plain}
\usepackage{amssymb,color}
\usepackage{amsfonts}
\usepackage{latexsym}
\usepackage{a4wide}
\usepackage{amsmath}

\newtheorem{theorem}{THEOREM}
\newtheorem{lemma}[theorem]{LEMMA}
\newtheorem{corollary}[theorem]{COROLLARY}
\newtheorem{proposition}[theorem]{PROPOSITION}
\newtheorem{remark}[theorem]{REMARK}
\newtheorem{definition}[theorem]{DEFINITION}
\newtheorem{fact}[theorem]{FACT}

\newtheorem{problem}[theorem]{PROBLEM}
\newtheorem{exercise}[theorem]{EXERCISE}
\def \set#1{\{#1\} }

\newenvironment{proof}{
PROOF:
\begin{quotation}}{
$\Box$ \end{quotation}}



\newcommand{\nats}{\mbox{\( \mathbb N \)}}
% \newcommand{\rat}{\mbox{\(\mathbb Q\)}}
\newcommand{\rats}{\mbox{\(\mathbb Q\)}}
\newcommand{\reals}{\mbox{\(\mathbb R\)}}
\newcommand{\ints}{\mbox{\(\mathbb Z\)}}

%%%%%%%%%%%%%%%%%%%%%%%%%%

% COVER PAGE;
\title{
  { \includegraphics[scale=.5]{ucl_logo.png}}\\
  {{\Huge Graph style transfer via Inverse Reinforcement Learning}}\\
  {\large An application to subway networks}\\
}

\date{Submission date: Day Month Year}
\author{
  Candidate Number: YLLK3\thanks{
      {\bf Disclaimer:}
      This report is submitted as part requirement 
      for the MSc Machine Learning at UCL. It is
      substantially the result of my own work except 
      where explicitly indicated in the text.
      The report may be freely copied and 
      distributed provided the source is explicitly acknowledged.
    }
    \\ \\
  MSc Machine Learning\\ \\
  Supervisor's name
}

\numberwithin{equation}{section}
\numberwithin{figure}{section}
\numberwithin{table}{section}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% START DOCUMENT;
\begin{document}
\onehalfspacing
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ABSTRACT;
\begin{abstract}
  Summarise your report concisely.
\end{abstract}
\tableofcontents
\setcounter{page}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - INTRODUCTION;
\chapter{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - RL BACKGROUND;
\chapter{Reinforcement Learning Background}
\label{chap:RL}
\textbf{TODO}: A SHORT SUMMARY OF THE FOLLOWING SENTENCES IS TO BE PUT 
IN THE END OF THE INTRO WHERE I EXPLAIN THE STRUCTURE OF THE 
REPORT.

In this section I cover the necessary background to follow the 
rest of the work. The main algorithm is model-free but has value 
functions and an explicit parameterised policy. I will not cover 
model-based RL in detail in what is to follow, therefore. The 
following will summarise the knowledge about the goal of RL, 
MDPs, Value functions, Policy gradients, off-policy learning 
and Actor-Critics. The last is particularly relevant, given 
that the policy learning algorithm in the inner loop of the IRL 
problem can be thought of as a ``soft'' variant of an Actor-Critic 
learning algorithm.

% SECTION - THE GOAL OF RL;
\section{The Goal of Reinforcement Learning}
\label{sec:RLGoal}

The goal of Reinforcement learning is to learn a behaviour that 
optimises some reward signal. The key assumption is that 
any goal (be it high-level or otherwise) can be fomulated as 
maximisation of the reward signal.

In the Reinforcement learning setup, we distinguish between two 
key entities - environment and agent. The environment is the 
entity that specifies a set of rules based on which, an action 
is judged given the environment's configuration. 
More specifically, let the configuration or state of the 
environment at time $t$ be denoted by $s_t$. Then, 
given an action, $a_t$ at time $t$, the environment evaluates 
how good this action is, given its 
current state, and outputs a (scalar) reward signal (or reward), 
$r_{t+1}$, and also updates its state to $s_{t+1}$. The agent 
is the entity responsible for giving an action $a_t$. 

It is worth noting that there are 
different conventions about indexing the reward, but in this work 
we adopt the convention from the Sutton and Barto book 
\citep{Sutton1998} where the reward is thought of as feedback 
given to the agent the step after the action was taken.

The agent may not always have access 
to the environment's state and can maintain its own state, based 
on which it takes an action. We refer to such settings as partially 
observed problems. 

An example of such a problem could be a robot 
whose goal is to find its way out of a maze as quickly as possible.
Suppose the robot only ``sees'' what is 
immediately surrounding it, in which case this would be its state 
or observation, $o_t$. 
In contrast, if it had access to the environment's state, it could 
see the shortest path out of the maze and take the necessary actions 
that yield the greatest accumulated reward (or return).

% SECTION - MDPs
\section{Markov Decision Processes (MDP)}
\label{sec:RLMDP}
Suppose the robot from the previous example has access to the 
environment state (sees the whole map). Then, given the 
current state of the environment 
and the action taken 
at that state, the next environment state does not depend on any 
past states and/or actions. That is, the 
environment state is first-order Markovian 
(given the present state and action). Indeed, this 
leads us to a convenint 
mathematical formulation of the dynamics of the 
Reinforcement learning problem called 
a Markov Decision Process (MDP).

An MDP with fully observed, Markovian states, is defined by the tuple 
($\mathcal{S},\; \mathcal{A},\; \mathcal{P},\; r(\cdot),\; \gamma$), where
\begin{itemize}
  \item $\mathcal{S}$ is the state space of the environment 
    (Markovian states).
  \item $\mathcal{A}$ is the action space (actions among which 
    the agent can select).
  \item $\mathcal{P}:\mathcal{S}\times \mathcal{A}\times \mathcal{S}\rightarrow [0, \infty)$ 
    is the transition operator and maps 
    the current state-action pair and the next state to the 
    correspondin probability density (in discrete state spaces, 
    the range will be $[0, 1]$).
  \item $r(\cdot)$ is the scalar-valued reward function 
    (usually defined as $r:\mathcal{S}\times \mathcal{A}\rightarrow \reals$, 
    although $r:\mathcal{S}\rightarrow \reals$ is also possible).
  \item $\gamma\in [0, 1]$ is the discount factor of rewards, which controls 
    how long or short sighted our objective is.
\end{itemize} 

The way $\gamma$ is used is typically alongside some function of 
the rewards collected along a path/trajectory. Let $\tau$ denote a 
path/trajectory given by:
\begin{equation}
  \tau:=(s_1, a_1, r_2, s_2, a_2, \ldots, s_T, a_T, r_{T+1}, s_{T+1}),\label{eq:tau} 
\end{equation} 
and $G:\mathcal{T}\rightarrow \reals$ denote the return 
function. Among the simplest examples of $G$ is the Monte-Carlo 
return:
\begin{equation}
  G(\tau):=\sum_{k=0}^{T-t}\gamma^kr_{t+k+1}.\label{eq:MCReturn}
\end{equation}
Looking at \eqref{eq:MCReturn}, we see that if we set $\gamma=0$ 
this corresponds to a myopic return, $G_t=r_{t+1}$, while if 
$\gamma=1$, we get a far-sighted return where we equally weigh 
all rewards along the trajectory $\tau$.

The MDP, as currently defined, induces a probabilistic graphical model 
(pgm) that is illustrated in Figure \ref{fig:MDP}. Due to D-separation 
we see that given $s_t$ and  $a_t$, $s_{t+1}$ is independent of 
$s_j$ and $a_j$ for all $j<t$.
Given a \textit{finite trajectory}/episode of length $T$, the induced 
factorisation is:
\begin{equation}
  p^{\pi}(\tau):=p(s_1)\prod_{t=1}^T \pi(a_t|s_t)p(s_{t+1}|s_t,a_t),
\end{equation}
where $\pi(\cdot)$ is known as the policy according to which the 
agent makes decisions and $p(s_{t+1}|s_t, a_t)$ is defined by the 
transition operator of the MDP, $\mathcal{P}$. Note the superscript 
in $p^{\pi}(\tau)$. This is there because we are giving the probability 
of the trajectory under the current behaviour of the agent, given 
by the policy $\pi$.

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,
      on grid , inner sep=0pt, minimum size=7mm,
      semithick, state/.style={circle, draw}]
      % s1, a1, s2;
      \node[state](s_1) at(0,0) {$s_1$};
      \node[state](a_1) at(1,1) {$a_1$};                                                                                                       
      \path (s_1) edge (a_1);
      \node[state](s_2) at(3,0) {$s_2$};
      \path (s_1) edge (s_2);
      \path (a_1) edge (s_2);
      % s2, a2, s3
      \node[state] (a_2) at(4,1) {$a_2$};
      \path (s_2) edge (a_2);
      \node[state] (s_3) at(6,0){$s_3$};
      \path (s_2) edge (s_3);
      \path (a_2) edge (s_3);
      % dots;
      \node (dots) at(7.5,0) {$\ldots$};
      \path (s_3) edge (dots);
  \end{tikzpicture}
  \caption{\label{fig:MDP} Graph of MDP with Markovian states.}
\end{figure}

As mentioned earlier with the robot example, we can have our agent 
not observe the Markovian state, and instead receive an observation, 
$o_t$, from its sensors. This induces a Partially Observed MDP (POMDP) 
whose graphical model is presented in Figure \ref{fig:POMDP}. As we 
can read off of the pgm in figure \ref{fig:POMDP}, we see that given 
$s_t$ and $a_t$, $s_{t+1}$ is still independent of 
$o_{1:t}$, $s_{1:t-1}$ and $a_{1:t-1}$ by D-separation. However,
The observations are not Markovian in general since $o_j$ for $j<t$ 
can influence $o_{t+1}$ even if $o_t$ and $a_t$ are known (again by 
D-separation). The observation is, however, independent of the past
givent the current state (unobserved in the POMDP setting).

\begin{figure}[H]
  \centering
  \begin{tikzpicture}[-latex ,auto ,node distance =1 cm and 1cm ,
      on grid , inner sep=0pt, minimum size=7mm,
      semithick, state/.style={circle, draw}]
      % s1, o1, a1, s2;
      \node[state](s_1) at(0,0) {$s_1$};
      \node[state](o_1) at (0, 1){$o_1$};
      \path (s_1) edge (o_1);
      \node[state](a_1) at(1.5,1) {$a_1$};
      \path (o_1) edge (a_1);
      \node[state](s_2) at(3,0) {$s_2$};
      \path (s_1) edge (s_2);
      \path (a_1) edge (s_2);
      % s2, o2, a2, s3
      \node[state](o_2) at (3,1) {$o_2$};
      \path (s_2) edge (o_2);
      \node[state] (a_2) at(4.5,1) {$a_2$};
      \path (o_2) edge (a_2);
      \node[state] (s_3) at(6,0){$s_3$};
      \path (s_2) edge (s_3);
      \path (a_2) edge (s_3);
      % dots;
      \node (dots) at(7.5,0) {$\ldots$};
      \path (s_3) edge (dots);
  \end{tikzpicture}
  \caption{\label{fig:POMDP} Graph of POMDP, $s_t$ is the Markovian 
  state, $o_t$ is the observation of the agent.}
\end{figure}

The associated factorisation based on the pgm in 
figure \ref{fig:POMDP} is:
\begin{equation}
  p^{\pi}(\tau)=p(s_1)\prod_{t=1}^T p(o_t|s_t)\pi(a_t|o_t)p(s_{t+1}|s_t, a_t).
\end{equation}
We see that in the POMDP setting, the agent makes decisions based 
on the observation and not the Markovian state. Depending on how 
informative the observation is, it may be difficult to construct 
a well-performing policy. Additionally, due to the non-Markovian 
nature of the observations, it is common to track and process old 
observations in order to form the agent state. Intuitively, one 
would expect the influence of observations from a long time ago 
to decay, so it might be useful to have a moving window over the 
collected experience.

In cases where we have deterministic transitions from state and action 
to the next state (and similarly from state to observation), 
$p(s_{t+1}|s_t, a_t)=\delta(s_{t+1},f(s_t, a_t))$, where 
$\delta(\cdot)$ denotes the delta distribution (placing all 
probability mass on $f(s_t, a_t)$) and 
$f: \mathcal{S}\times\mathcal{A}\rightarrow \mathcal{S}$ is some deterministic 
transition function (similarly $g:\mathcal{S}\rightarrow \mathcal{O}$ 
can be the sensor giving the observation).

% SECTION - FORMULATING EPISODIC AND CONTINUAL SETTING REWARD;
\section{Formulating objective functions}
\label{sec:RLObjectives}
\begin{itemize}
  \item Talk about episodic and continual settings;
  \item Talk about value functions;
\end{itemize}

% SECTION - improving the policy;
\section{title}



\begin{itemize}
  \item Define the RL objective - optimising long term reward 
    or the episodic objective with the returns;
  \item Give the Bellman equations as a means to solve an MDP;
    Give proof of the convergence of those Bellman operators;
  \item Make the claim about the existence of an optimal greedy 
    policy, given an MDP;
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTHER - IRL THEORY;
\chapter{Inverse Reinforcement Learning}
Test citation \citep{NgIRL} - done.
\begin{itemize}
  \item State the goal of IRL - estimate an objective function
    that when optimised should yield a behaviour similar to the 
    one in the demos by the expert.
  \item Argue the point about the ill-posedness of IRL, which makes 
    classic optimal control not suitable for the job.
  \item Talk about the max-margin approach (Ng) to IRL.
    State drawbacks;
  \item Talk about the max-entropy Ziebart approach;
    Show the (soft) ``duality'' between this probabilistic setting and 
    the updates from the Bellman equations - see Levine lectures 
    and motivations and his review paper. Here should talk about the 
    works of Todorov, Sergey's review paper and SAC as a means to 
    work in this probabilistic setting.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - GRAPH REPRESENTATIONS;
\chapter{Graph representations}
\begin{itemize}
  \item Give a lit review with the Scarsalli (can be enough to just 
  mention his name and give the general MPNN structure).
  \item Talk about the applications of GNNs - e.g., the Kipf paper 
    with GCN and application to citation graphs;
  \item Talk about the GATv2 which I am using.
  \item Read/skim the new paper about how much can gnns learn.
  \item State drawbacks and potential reasons, why it is difficult 
    to work with graph topography rather than topology when using 
    GNNs.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - OUR CONTRIBUTION;
\chapter{Our Contribution}
\begin{itemize}
  \item Basically the different spins on GrphOpt;
    \subitem Check effect of adding separate reward encoder;
    \subitem Check effect of UT trick - didn't seem so good, might drop;
    \subitem Check effect of multitask training - see if works;
    \subitem Talk about particular implementation experience
      gathering and storing it in the Replay Buffer; talk about 
      the batch-sampling time reward computation and not wasting 
      state-action trajectory experience by having to clear the 
      buffer after reward update, had we stayed with vanilla description;
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NEW CHAPTER - EXPERIMENTS;
\chapter{Experiments}
\begin{itemize}
  \item T1 is about transferring learned reward on target graph;
  also compares learned policy directly on target, compared to 
  training new policy on target with frozen reward;
  \item T2 is about MRR - based on top 25 proposal edges;;
  \item T3 is about running learned policy on source graph and 
  seeing if we get similar graphs;
  \item I also get rewards along paths.
  \begin{itemize}
    \item Compare convergence rate of eval returns over seeds
      and comment on speed;
  \end{itemize}
  \item BA40 to BA80 - 10 seeds
  \begin{itemize}
    \item GOptEpWeights, GOptPerDec, 
      GOptRewEncEpWeights, GOptRewEncPerDec,
    \item THIS IS NOT APPLICABLE TO BA GRAPHS - ONLY TRY IT ON GRAPHS 
      WITH MEANINGFUL TOPOGRAPHY!!! - Then take better of EpWeights and PerDec and do 
      GOptMultitaskBetterWeight and GOptRewEncMultitaskBetterWeight,
    \item also run UT trick with the better weight setting;
  \end{itemize}
    
  \item T1 and T3 run together - this is $10\times 8=80$ experiments;
  \item T2 runs alone - this is $10\times 8=80$ experiments;
  \item total is 160 experiments;
  \item  Run best config on Urban nets 10 times;
\end{itemize}
\chapter{Conclusion and Further work}


\chapter{title of first chapter}
This is just a bare minimum to get started.  There is unlimited guidance on using latex, e.g. {\tt https://en.wikibooks.org/wiki/LaTeX}.   You are still responsible to check the detailed requirements of a project, including formatting instructions, see \\
{\tt https://moodle.ucl.ac.uk/pluginfile.php/3591429/mod\_resource/content/7/UGProjects2017.pdf}.
Leave at least a line of white space when you want to start a new paragraph.

Mathematical expressions are placed inline between dollar signs, e.g. $\sqrt 2, \sum_{i=0}^nf(i)$, or in display mode
\[ e^{i\pi}=-1\] and another way, this time with labels,
\begin{align}
\label{line1} A=B\wedge B=C&\rightarrow A=C\\
&\rightarrow C=A\\
\intertext{note that}
n!&=\prod_{1\leq i\leq n}i \\
\int_{x=1}^y \frac 1 x \mathrm{d}x&=\log y
\end{align}
We can refer to labels like this \eqref{line1}.   

\chapter{title of second chapter}
% Often lots of citations here (and elsewhere), e.g. \cite{Rey:D} or \cite[Theorem 2.3]{PriorNOP70}.   Bibtex can help with this, but is not essential. If you want pictures, try

\begin{center}
\includegraphics[scale=.5]{aristotle.jpg}
\end{center}
You can use 
\begin{itemize}
\item lists
\item like this
\end{itemize}
or numbered
\begin{enumerate}
\item like this,
\item or this
\end{enumerate}
but don't overdo it.
\chapter{title of third chapter}
If you have a formal theorem you might try this.
\begin{definition}\label{def}
See definition~\ref{def}.
\end{definition}
\begin{theorem}
For all $n\in\nats,\; 1^n=1$.
\end{theorem}
\begin{proof}
By induction over $n$.
\end{proof}

\chapter{etc.}
\appendix

\bibliographystyle{apa}
\bibliography{my_bib.bib}

\end{document}