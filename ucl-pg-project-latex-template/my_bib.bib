@misc{NgIRL,
  author = {Ng, Andrew Y. and Russell, Stuart J.},
  booktitle = {ICML},
  date = {2002-11-26},
  pages = {663-670},
  title = {Algorithms for Inverse Reinforcement Learning.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2000.html#NgR00},
  year = 2000
}

@inproceedings{Ziebart2008,
  added-at = {2012-12-10T00:00:00.000+0100},
  author = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
  biburl = {https://www.bibsonomy.org/bibtex/2231d8449e296188b91673570bc70b1d6/dblp},
  booktitle = {AAAI},
  crossref = {conf/aaai/2008},
  editor = {Fox, Dieter and Gomes, Carla P.},
  ee = {http://www.aaai.org/Library/AAAI/2008/aaai08-227.php},
  interhash = {b7a2d62a0a6b56f0bf94f4392f8f445a},
  intrahash = {231d8449e296188b91673570bc70b1d6},
  isbn = {978-1-57735-368-3},
  keywords = {dblp},
  pages = {1433-1438},
  publisher = {AAAI Press},
  timestamp = {2012-12-11T11:46:42.000+0100},
  title = {Maximum Entropy Inverse Reinforcement Learning.},
  url = {http://dblp.uni-trier.de/db/conf/aaai/aaai2008.html#ZiebartMBD08},
  year = 2008
}
@article{LevineRLasInf,
  abstract = {The framework of reinforcement learning or optimal control provides a
mathematical formalization of intelligent decision making that is powerful and
broadly applicable. While the general form of the reinforcement learning
problem enables effective reasoning about uncertainty, the connection between
reinforcement learning and inference in probabilistic models is not immediately
obvious. However, such a connection has considerable value when it comes to
algorithm design: formalizing a problem as probabilistic inference in principle
allows us to bring to bear a wide array of approximate inference tools, extend
the model in flexible and powerful ways, and reason about compositionality and
partial observability. In this article, we will discuss how a generalization of
the reinforcement learning or optimal control problem, which is sometimes
termed maximum entropy reinforcement learning, is equivalent to exact
probabilistic inference in the case of deterministic dynamics, and variational
inference in the case of stochastic dynamics. We will present a detailed
derivation of this framework, overview prior work that has drawn on this and
related ideas to propose new reinforcement learning and control algorithms, and
describe perspectives on future research.},
  added-at = {2019-07-22T04:03:13.000+0200},
  author = {Levine, Sergey},
  biburl = {https://www.bibsonomy.org/bibtex/27c4d487d33f50d403f91252d71577962/kirk86},
  description = {[1805.00909] Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
  interhash = {844c303a322030e12b3e34d823393c58},
  intrahash = {7c4d487d33f50d403f91252d71577962},
  keywords = {reinforcement-learning tutorials},
  note = {cite arxiv:1805.00909},
  timestamp = {2019-07-22T04:03:13.000+0200},
  title = {Reinforcement Learning and Control as Probabilistic Inference: Tutorial
  and Review},
  url = {http://arxiv.org/abs/1805.00909},
  year = 2018
}
@inproceedings{FinnGCL,
  added-at = {2019-05-29T00:00:00.000+0200},
  author = {Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  biburl = {https://www.bibsonomy.org/bibtex/29dbf4c44fddfa4305fac4f566b21f547/dblp},
  booktitle = {ICML},
  crossref = {conf/icml/2016},
  editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
  ee = {http://proceedings.mlr.press/v48/finn16.html},
  interhash = {365aecb78318e1c7dc11396d12f8feb0},
  intrahash = {9dbf4c44fddfa4305fac4f566b21f547},
  keywords = {dblp},
  pages = {49-58},
  publisher = {JMLR.org},
  series = {JMLR Workshop and Conference Proceedings},
  timestamp = {2019-05-30T11:52:41.000+0200},
  title = {Guided Cost Learning: Deep Inverse Optimal Control via Policy Optimization.},
  url = {http://dblp.uni-trier.de/db/conf/icml/icml2016.html#FinnLA16},
  volume = 48,
  year = 2016
}
@article{GraphOpt,
  added-at = {2020-07-20T00:00:00.000+0200},
  author = {Trivedi, Rakshit and Yang, Jiachen and Zha, Hongyuan},
  biburl = {https://www.bibsonomy.org/bibtex/28f6359a96da094add27998cbbeb64836/dblp},
  ee = {https://arxiv.org/abs/2007.03619},
  interhash = {4c9e9ebb5be97f05334f9ba6f13ed009},
  intrahash = {8f6359a96da094add27998cbbeb64836},
  journal = {CoRR},
  keywords = {dblp},
  timestamp = {2020-07-24T00:28:21.000+0200},
  title = {GraphOpt: Learning Optimization Models of Graph Formation.},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr2007.html#abs-2007-03619},
  volume = {abs/2007.03619},
  year = 2020
}
@inproceedings{NerveNet,
  added-at = {2019-07-25T00:00:00.000+0200},
  author = {Wang, Tingwu and Liao, Renjie and Ba, Jimmy and Fidler, Sanja},
  biburl = {https://www.bibsonomy.org/bibtex/27094e296a3041c368a28f2f65ebfc9b8/dblp},
  booktitle = {ICLR (Poster)},
  crossref = {conf/iclr/2018},
  ee = {https://openreview.net/forum?id=S1sqHMZCb},
  interhash = {2798f33fac352cc3b0688e8eb6cb8c00},
  intrahash = {7094e296a3041c368a28f2f65ebfc9b8},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2019-07-26T11:43:51.000+0200},
  title = {NerveNet: Learning Structured Policy with Graph Neural Networks.},
  url = {http://dblp.uni-trier.de/db/conf/iclr/iclr2018.html#WangLBF18},
  year = 2018
}
@misc{oversquashingGNN,
      title={How does over-squashing affect the power of GNNs?}, 
      author={Francesco Di Giovanni and T. Konstantin Rusch and Michael M. Bronstein and Andreea Deac and Marc Lackenby and Siddhartha Mishra and Petar Veličković},
      year={2023},
      eprint={2306.03589},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{HasseltDoubleQlearning,
      title={Deep Reinforcement Learning with Double Q-learning}, 
      author={Hado van Hasselt and Arthur Guez and David Silver},
      year={2015},
      eprint={1509.06461},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{SAC1,
  title = 	 {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author =       {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1861--1870},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf},
  url = 	 {https://proceedings.mlr.press/v80/haarnoja18b.html},
  abstract = 	 {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.}
}
@misc{SAC2,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  keywords = {},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018 }
}