{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -2.6603, -10.1530,  -3.0829])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributions as dists\n",
    "\n",
    "d = dists.Normal(torch.zeros((3, 10)), torch.ones(3, 10))\n",
    "\n",
    "def f(x, d):\n",
    "    return (-.5 * ((x - d.mean) / d.stddev) ** 2).sum(-1)\n",
    "\n",
    "f(torch.randn((3, 10)), d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.8.17)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/home/mario/coding/urban-nets-style-transfer/tests\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "p = Path('.').absolute().parent\n",
    "if sys.path[-1] != str(p):\n",
    "    sys.path.append(str(p))\n",
    "\n",
    "from graph_irl.buffer_v2 import GraphBuffer\n",
    "from graph_irl.policy import GaussPolicy, TwoStageGaussPolicy, GCN, Qfunc\n",
    "from graph_irl.graph_rl_utils import GraphEnv\n",
    "from graph_irl.sac import SACAgentGraph, TEST_OUTPUTS_PATH\n",
    "from graph_irl.reward import GraphReward\n",
    "from graph_irl.examples.circle_graph import create_circle_graph\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "# circular graph with 7 nodes;\n",
    "n_nodes, node_dim = 11, 5\n",
    "nodes, edge_index = create_circle_graph(n_nodes, node_dim, torch.ones)\n",
    "encoder_hiddens = [7, 7, 2]\n",
    "\n",
    "encoder = GCN(node_dim, encoder_hiddens, with_layer_norm=True, final_tanh=True)\n",
    "encoderq1 = GCN(node_dim, encoder_hiddens, with_layer_norm=True, final_tanh=True)\n",
    "encoderq2 = GCN(node_dim, encoder_hiddens, with_layer_norm=True, final_tanh=True)\n",
    "encoderq1t = GCN(node_dim, encoder_hiddens, with_layer_norm=True, final_tanh=True)\n",
    "encoderq2t = GCN(node_dim, encoder_hiddens, with_layer_norm=True, final_tanh=True)\n",
    "encoder_reward = GCN(node_dim, encoder_hiddens, with_layer_norm=True, final_tanh=True)\n",
    "reward_fn = GraphReward(encoder_reward, encoder_hiddens[-1], [5, 5], with_layer_norm=True)\n",
    "\n",
    "gauss_policy_kwargs = dict(\n",
    "    obs_dim=encoder_hiddens[-1],\n",
    "    action_dim=encoder_hiddens[-1],\n",
    "    hiddens=[5, 5],\n",
    "    with_layer_norm=True,\n",
    "    encoder=encoder,\n",
    "    two_action_vectors=True,\n",
    ")\n",
    "\n",
    "tsg_policy_kwargs = dict(\n",
    "    obs_dim=encoder_hiddens[-1],\n",
    "    action_dim=encoder_hiddens[-1],\n",
    "    hiddens1=[5, 5],\n",
    "    hiddens2=[7, 7],\n",
    "    encoder=encoder,\n",
    "    with_layer_norm=True,\n",
    ")\n",
    "\n",
    "qfunc_kwargs = dict(\n",
    "    obs_action_dim=encoder_hiddens[-1] * 3, \n",
    "    hiddens=[5, 5], \n",
    "    with_layer_norm=True, \n",
    "    encoder=None\n",
    ")\n",
    "\n",
    "Q1_kwargs = qfunc_kwargs.copy()\n",
    "Q1_kwargs['encoder'] = encoderq1\n",
    "Q2_kwargs = qfunc_kwargs.copy()\n",
    "Q2_kwargs['encoder'] = encoderq2\n",
    "Q1t_kwargs = qfunc_kwargs.copy()\n",
    "Q1t_kwargs['encoder'] = encoderq1t\n",
    "Q2t_kwargs = qfunc_kwargs.copy()\n",
    "Q2t_kwargs['encoder'] = encoderq2t\n",
    "\n",
    "agent_kwargs=dict(\n",
    "    name='SACAgentGraph',\n",
    "    policy_constructor=GaussPolicy,\n",
    "    qfunc_constructor=Qfunc,\n",
    "    env_constructor=GraphEnv,\n",
    "    buffer_constructor=GraphBuffer,\n",
    "    optimiser_constructors=dict(\n",
    "        policy_optim=torch.optim.SGD,\n",
    "        temperature_optim=torch.optim.SGD,\n",
    "        Q1_optim=torch.optim.SGD,\n",
    "        Q2_optim=torch.optim.SGD,\n",
    "    ),\n",
    "    entropy_lb=encoder_hiddens[-1],\n",
    "    policy_lr=1e-3,\n",
    "    temperature_lr=1e-3,\n",
    "    qfunc_lr=1e-3,\n",
    "    tau=0.005,\n",
    "    discount=1.,\n",
    "    save_to=TEST_OUTPUTS_PATH,\n",
    "    cache_best_policy=False,\n",
    "    clip_grads=True,\n",
    "    UT_trick=False,\n",
    "    with_entropy=False,\n",
    ")\n",
    "\n",
    "config = dict(\n",
    "    training_kwargs=dict(\n",
    "        seed=0,\n",
    "        num_iters=50,\n",
    "        num_steps_to_sample=100,\n",
    "        num_grad_steps=1,\n",
    "        batch_size=100,\n",
    "        num_eval_steps_to_sample=n_nodes,\n",
    "        min_steps_to_presample=100,\n",
    "    ),\n",
    "    Q1_kwargs=Q1_kwargs,\n",
    "    Q2_kwargs=Q2_kwargs,\n",
    "    Q1t_kwargs=Q1t_kwargs,\n",
    "    Q2t_kwargs=Q2t_kwargs,\n",
    "    policy_kwargs=gauss_policy_kwargs,\n",
    "    buffer_kwargs=dict(\n",
    "        max_size=10_000,\n",
    "        nodes=nodes,\n",
    "        seed=0,\n",
    "        drop_repeats_or_self_loops=True,\n",
    "        get_batch_reward=True,\n",
    "        graphs_per_batch=100,\n",
    "        action_is_index=True,\n",
    "        per_decision_imp_sample=False,\n",
    "    ),\n",
    "    env_kwargs=dict(\n",
    "        x=nodes,\n",
    "        reward_fn=reward_fn,\n",
    "        max_episode_steps=n_nodes,\n",
    "        num_expert_steps=n_nodes,\n",
    "        max_repeats=n_nodes // 3,\n",
    "        max_self_loops=n_nodes // 3,\n",
    "        drop_repeats_or_self_loops=True,\n",
    "        id=None,\n",
    "        reward_fn_termination=False,\n",
    "        calculate_reward=False,\n",
    "        min_steps_to_do=3,\n",
    "    )\n",
    ")\n",
    "\n",
    "agent = SACAgentGraph(\n",
    "    **agent_kwargs,\n",
    "    **config\n",
    ")\n",
    "\n",
    "# print(agent.buffer.idx, agent.buffer.reward_idx)\n",
    "# agent.buffer.collect_path(agent.env, agent, agent.num_steps_to_sample)\n",
    "# print(agent.buffer.idx, agent.buffer.reward_idx, \n",
    "#       np.mean(agent.buffer.path_lens), \n",
    "#       np.max(agent.buffer.path_lens), \n",
    "#       agent.buffer.path_lens,\n",
    "#       agent.buffer.reward_t[:agent.buffer.reward_idx],\n",
    "#       sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agent\u001b[39m.\u001b[39;49mbuffer\u001b[39m.\u001b[39;49mget_single_ep_rewards_and_weights(agent\u001b[39m.\u001b[39;49menv, agent, \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/coding/urban-nets-style-transfer/graph_irl/buffer_v2.py:222\u001b[0m, in \u001b[0;36mGraphBuffer.get_single_ep_rewards_and_weights\u001b[0;34m(self, env, agent, lcr_reg, verbose)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[39m# process the episode;\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m (terminated \u001b[39mor\u001b[39;00m truncated):\n\u001b[1;32m    220\u001b[0m     \n\u001b[1;32m    221\u001b[0m     \u001b[39m# sample stochastic actions;\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m     (a1, a2), node_embeds \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49msample_action(obs)\n\u001b[1;32m    224\u001b[0m     \u001b[39m# make env step;\u001b[39;00m\n\u001b[1;32m    225\u001b[0m     new_obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(\n\u001b[1;32m    226\u001b[0m         ((a1\u001b[39m.\u001b[39mnumpy(), a2\u001b[39m.\u001b[39mnumpy()), node_embeds\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m    227\u001b[0m     )\n",
      "File \u001b[0;32m~/coding/urban-nets-style-transfer/graph_irl/sac.py:375\u001b[0m, in \u001b[0;36mSACAgentGraph.sample_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    374\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample_action\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[0;32m--> 375\u001b[0m     policy_dist, node_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(obs)\n\u001b[1;32m    376\u001b[0m     \u001b[39mreturn\u001b[39;00m policy_dist\u001b[39m.\u001b[39msample(), node_embeds\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env-conda-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/coding/urban-nets-style-transfer/graph_irl/policy.py:140\u001b[0m, in \u001b[0;36mGaussPolicy.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, obs):\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m         obs, node_embeds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(obs)  \u001b[39m# (B, obs_dim)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     mus, sigmas \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet(obs)\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env-conda-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/coding/urban-nets-style-transfer/graph_irl/policy.py:46\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     43\u001b[0m x, edge_index \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mx, batch\u001b[39m.\u001b[39medge_index\n\u001b[1;32m     44\u001b[0m \u001b[39mfor\u001b[39;00m i, f \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet):\n\u001b[1;32m     45\u001b[0m     \u001b[39m# GNN pass;\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m     x \u001b[39m=\u001b[39m f(x, edge_index)\n\u001b[1;32m     48\u001b[0m     \u001b[39m# activation follow-up;\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env-conda-py38/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env-conda-py38/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:210\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;49;00m\n\u001b[1;32m    211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_dim),\n\u001b[1;32m    212\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimproved, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_self_loops, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflow, x\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env-conda-py38/lib/python3.8/site-packages/torch_geometric/nn/conv/gcn_conv.py:100\u001b[0m, in \u001b[0;36mgcn_norm\u001b[0;34m(edge_index, edge_weight, num_nodes, improved, add_self_loops, flow, dtype)\u001b[0m\n\u001b[1;32m     98\u001b[0m row, col \u001b[39m=\u001b[39m edge_index[\u001b[39m0\u001b[39m], edge_index[\u001b[39m1\u001b[39m]\n\u001b[1;32m     99\u001b[0m idx \u001b[39m=\u001b[39m col \u001b[39mif\u001b[39;00m flow \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msource_to_target\u001b[39m\u001b[39m'\u001b[39m \u001b[39melse\u001b[39;00m row\n\u001b[0;32m--> 100\u001b[0m deg \u001b[39m=\u001b[39m scatter(edge_weight, idx, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, dim_size\u001b[39m=\u001b[39;49mnum_nodes, reduce\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    101\u001b[0m deg_inv_sqrt \u001b[39m=\u001b[39m deg\u001b[39m.\u001b[39mpow_(\u001b[39m-\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m    102\u001b[0m deg_inv_sqrt\u001b[39m.\u001b[39mmasked_fill_(deg_inv_sqrt \u001b[39m==\u001b[39m \u001b[39mfloat\u001b[39m(\u001b[39m'\u001b[39m\u001b[39minf\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-env-conda-py38/lib/python3.8/site-packages/torch_geometric/utils/scatter.py:74\u001b[0m, in \u001b[0;36mscatter\u001b[0;34m(src, index, dim, dim_size, reduce)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39madd\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     73\u001b[0m     index \u001b[39m=\u001b[39m broadcast(index, src, dim)\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m src\u001b[39m.\u001b[39;49mnew_zeros(size)\u001b[39m.\u001b[39mscatter_add_(dim, index, src)\n\u001b[1;32m     76\u001b[0m \u001b[39mif\u001b[39;00m reduce \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     77\u001b[0m     count \u001b[39m=\u001b[39m src\u001b[39m.\u001b[39mnew_zeros(dim_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.buffer.get_single_ep_rewards_and_weights(agent.env, agent, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.buffer.idx, agent.buffer.reward_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_irl.irl_trainer import IRLGraphTrainer\n",
    "\n",
    "a = [0] + torch.repeat_interleave(torch.arange(1, n_nodes), 2).tolist() + [0]\n",
    "\n",
    "expert_edge_index = torch.tensor([\n",
    "    a,\n",
    "    ((torch.tensor([1, -1] * (len(a) // 2)) + torch.tensor(a)) % n_nodes).tolist()\n",
    "], dtype=torch.long)\n",
    "\n",
    "irl_trainer = IRLGraphTrainer(\n",
    "    reward_fn=reward_fn,\n",
    "    reward_optim=torch.optim.Adam(reward_fn.parameters(), lr=1e-3),\n",
    "    agent=agent,\n",
    "    nodes=nodes,\n",
    "    expert_edge_index=expert_edge_index,\n",
    "    num_expert_traj=5, \n",
    "    # num_generated_traj=5,\n",
    "    graphs_per_batch=config['buffer_kwargs']['graphs_per_batch'], \n",
    "    reward_optim_lr_scheduler=None,\n",
    "    reward_grad_clip=False,\n",
    "    reward_scale=1.,\n",
    "    per_decision_imp_sample=config['buffer_kwargs']['per_decision_imp_sample'],\n",
    "    add_expert_to_generated=False,\n",
    "    lcr_regularisation_coef=None,\n",
    "    mono_regularisation_on_demo_coef=None,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# irl_trainer = IRLGraphTrainer(\n",
    "    # reward_fn,\n",
    "    # reward_optim,\n",
    "    # agent,\n",
    "    # nodes,\n",
    "    # expert_edge_index,\n",
    "    # num_expert_traj,\n",
    "    # graphs_per_batch,\n",
    "    # reward_optim_lr_scheduler=None,\n",
    "    # reward_grad_clip=False,\n",
    "    # reward_scale=1.,\n",
    "    # per_decision_imp_sample=False,\n",
    "    # add_expert_to_generated=False,\n",
    "    # lcr_regularisation_coef=None,\n",
    "    # mono_regularisation_on_demo_coef=None,\n",
    "    # verbose=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:14<00:00,  3.49it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.55s/it]\n"
     ]
    }
   ],
   "source": [
    "irl_trainer.train_policy_k_epochs(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 tensor(0.7223, grad_fn=<AddBackward0>)\n",
      "7 tensor(1.1016, grad_fn=<AddBackward0>)\n",
      "4 tensor(0.4583, grad_fn=<AddBackward0>)\n",
      "10 tensor(5.0503, grad_fn=<AddBackward0>)\n",
      "6 tensor(0.5155, grad_fn=<AddBackward0>)\n",
      "10 tensor(1.9955, grad_fn=<AddBackward0>)\n",
      "6 tensor(1.0445, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(nan, grad_fn=<SumBackward0>), tensor(1.5554, grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irl_trainer._get_per_dec_imp_samp_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights vanilla\n",
      "[tensor(4.5529e+11), tensor(7.4860e+12), tensor(5.6435e+19), tensor(1.0507e+17), tensor(4.5316e+16), tensor(5.8864e+16)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(-1.8085, grad_fn=<SumBackward0>), 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "irl_trainer._get_vanilla_imp_sampled_returns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights vanilla\n",
      "[tensor(8.4403e+19), tensor(4.7524e+14), tensor(3.9331e+08), tensor(2.5771e+15), tensor(707796.2500), tensor(1.3794e+18), tensor(6.6347e+16)]\n",
      "hi\n",
      "expert avg rewards: -2.7328226566314697\n",
      "imp sampled rewards: -4.173259258270264\n",
      "mono loss: 0.0\n",
      "lcr_expert_loss: 0.0\n",
      "lcr_sampled_loss: 0.0\n",
      "overall reward loss: -1.440436601638794\n",
      "len module param: 7 l2 norm of grad of params: 454.623046875\n",
      "len module param: 7 l2 norm of grad of params: 855.0362548828125\n",
      "len module param: 7 l2 norm of grad of params: 308.23040771484375\n",
      "len module param: 7 l2 norm of grad of params: 818.640625\n",
      "len module param: 2 l2 norm of grad of params: 12.563790321350098\n",
      "len module param: 2 l2 norm of grad of params: 113.96983337402344\n",
      "len module param: 5 l2 norm of grad of params: 259.1490478515625\n",
      "len module param: 5 l2 norm of grad of params: 195.12933349609375\n",
      "len module param: 5 l2 norm of grad of params: 0.9488996267318726\n",
      "len module param: 5 l2 norm of grad of params: 3.7890665531158447\n",
      "len module param: 5 l2 norm of grad of params: 6.008973598480225\n",
      "len module param: 5 l2 norm of grad of params: 5.64832878112793\n",
      "len module param: 5 l2 norm of grad of params: 1.8065168857574463\n",
      "len module param: 5 l2 norm of grad of params: 0.8631459474563599\n",
      "len module param: 1 l2 norm of grad of params: 7.008488655090332\n",
      "len module param: 1 l2 norm of grad of params: 1.365410327911377\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "irl_trainer.do_reward_grad_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights vanilla\n",
      "[tensor(5.6648e+13), tensor(3.4684e+18), tensor(2.0338e+20), tensor(2.0412e+17), tensor(7.7866e+18), tensor(1.2702e+19)]\n",
      "hi\n",
      "expert avg rewards: -2.25759220123291\n",
      "imp sampled rewards: -1.1205915212631226\n",
      "mono loss: 0.0\n",
      "lcr_expert_loss: 0.0\n",
      "lcr_sampled_loss: 0.0\n",
      "overall reward loss: 1.1370006799697876\n",
      "len module param: 7 l2 norm of grad of params: 7.995495319366455\n",
      "len module param: 7 l2 norm of grad of params: 18.172767639160156\n",
      "len module param: 7 l2 norm of grad of params: 29.354124069213867\n",
      "len module param: 7 l2 norm of grad of params: 15.44437313079834\n",
      "len module param: 2 l2 norm of grad of params: 1.6760761737823486\n",
      "len module param: 2 l2 norm of grad of params: 2.1715078353881836\n",
      "len module param: 5 l2 norm of grad of params: 13.020103454589844\n",
      "len module param: 5 l2 norm of grad of params: 10.493112564086914\n",
      "len module param: 5 l2 norm of grad of params: 1.2404022216796875\n",
      "len module param: 5 l2 norm of grad of params: 2.262253999710083\n",
      "len module param: 5 l2 norm of grad of params: 5.616767406463623\n",
      "len module param: 5 l2 norm of grad of params: 3.5853400230407715\n",
      "len module param: 5 l2 norm of grad of params: 0.9273566603660583\n",
      "len module param: 5 l2 norm of grad of params: 1.336312174797058\n",
      "len module param: 1 l2 norm of grad of params: 3.5765700340270996\n",
      "len module param: 1 l2 norm of grad of params: 2.1089911460876465\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 50/50 [00:14<00:00,  3.40it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.72s/it]\n",
      "/home/mario/coding/urban-nets-style-transfer/graph_irl/vis_utils.py:90: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n",
      "  ax.set_ylim(ylow, yhigh)\n",
      " 50%|█████     | 1/2 [00:15<00:15, 15.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights vanilla\n",
      "[tensor(7.1979e+17), tensor(1.4116e+08), tensor(1.1959e+19), tensor(1.9929e+15), tensor(24201576.), tensor(7.7659e+18), tensor(2.4562e+18)]\n",
      "hi\n",
      "expert avg rewards: -1.7531073093414307\n",
      "imp sampled rewards: -1.7056950330734253\n",
      "mono loss: 0.0\n",
      "lcr_expert_loss: 0.0\n",
      "lcr_sampled_loss: 0.0\n",
      "overall reward loss: 0.04741227626800537\n",
      "len module param: 7 l2 norm of grad of params: 14.223734855651855\n",
      "len module param: 7 l2 norm of grad of params: 22.31707000732422\n",
      "len module param: 7 l2 norm of grad of params: 83.16067504882812\n",
      "len module param: 7 l2 norm of grad of params: 15.936647415161133\n",
      "len module param: 2 l2 norm of grad of params: 1.0145635604858398\n",
      "len module param: 2 l2 norm of grad of params: 6.396261215209961\n",
      "len module param: 5 l2 norm of grad of params: 13.428030014038086\n",
      "len module param: 5 l2 norm of grad of params: 3.036083936691284\n",
      "len module param: 5 l2 norm of grad of params: 2.091089963912964\n",
      "len module param: 5 l2 norm of grad of params: 2.1539509296417236\n",
      "len module param: 5 l2 norm of grad of params: 8.056299209594727\n",
      "len module param: 5 l2 norm of grad of params: 3.4916024208068848\n",
      "len module param: 5 l2 norm of grad of params: 0.798326313495636\n",
      "len module param: 5 l2 norm of grad of params: 0.4093308746814728\n",
      "len module param: 1 l2 norm of grad of params: 4.267062664031982\n",
      "len module param: 1 l2 norm of grad of params: 0.6463510990142822\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 50/50 [00:14<00:00,  3.55it/s]\n",
      "100%|██████████| 1/1 [00:14<00:00, 14.10s/it]\n",
      "100%|██████████| 2/2 [00:30<00:00, 15.42s/it]\n"
     ]
    }
   ],
   "source": [
    "irl_trainer.train_irl(2, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env-conda-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
