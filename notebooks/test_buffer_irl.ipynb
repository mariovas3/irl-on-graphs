{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.0 (SDL 2.28.0, Python 3.8.17)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "/home/mario/coding/urban-nets-style-transfer/tests\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "p = Path('.').absolute().parent\n",
    "if sys.path[-1] != str(p):\n",
    "    sys.path.append(str(p))\n",
    "\n",
    "from graph_irl.buffer_v2 import GraphBuffer\n",
    "from graph_irl.policy import GaussPolicy, TwoStageGaussPolicy, GCN, Qfunc\n",
    "from graph_irl.graph_rl_utils import GraphEnv\n",
    "from graph_irl.sac import SACAgentGraph, TEST_OUTPUTS_PATH\n",
    "from graph_irl.reward import GraphReward\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "n_nodes = 10\n",
    "nodes = torch.randn((n_nodes, 5))\n",
    "encoder_hiddens = [3, 7]\n",
    "\n",
    "encoder = GCN(5, encoder_hiddens, with_layer_norm=True, final_tanh=False)\n",
    "encoderq1 = GCN(5, encoder_hiddens, with_layer_norm=True, final_tanh=False)\n",
    "encoderq2 = GCN(5, encoder_hiddens, with_layer_norm=True, final_tanh=False)\n",
    "encoderq1t = GCN(5, encoder_hiddens, with_layer_norm=True, final_tanh=False)\n",
    "encoderq2t = GCN(5, encoder_hiddens, with_layer_norm=True, final_tanh=False)\n",
    "encoder_reward = GCN(5, encoder_hiddens, with_layer_norm=True, final_tanh=False)\n",
    "reward_fn = GraphReward(encoder_reward, encoder_hiddens[-1], [23, 23], with_layer_norm=True)\n",
    "\n",
    "gauss_policy_kwargs = dict(\n",
    "    obs_dim=encoder_hiddens[-1],\n",
    "    action_dim=encoder_hiddens[-1],\n",
    "    hiddens=[13, 13],\n",
    "    with_layer_norm=False,\n",
    "    encoder=encoder,\n",
    "    two_action_vectors=True,\n",
    ")\n",
    "\n",
    "tsg_policy_kwargs = dict(\n",
    "    obs_dim=encoder_hiddens[-1],\n",
    "    action_dim=encoder_hiddens[-1],\n",
    "    hiddens1=[13, 13],\n",
    "    hiddens2=[11, 11],\n",
    "    encoder=encoder,\n",
    "    with_layer_norm=True,\n",
    ")\n",
    "\n",
    "qfunc_kwargs = dict(\n",
    "    obs_action_dim=encoder_hiddens[-1] * 3, \n",
    "    hiddens=[17, 17], \n",
    "    with_layer_norm=True, \n",
    "    encoder=None\n",
    ")\n",
    "\n",
    "Q1_kwargs = qfunc_kwargs.copy()\n",
    "Q1_kwargs['encoder'] = encoderq1\n",
    "Q2_kwargs = qfunc_kwargs.copy()\n",
    "Q2_kwargs['encoder'] = encoderq2\n",
    "Q1t_kwargs = qfunc_kwargs.copy()\n",
    "Q1t_kwargs['encoder'] = encoderq1t\n",
    "Q2t_kwargs = qfunc_kwargs.copy()\n",
    "Q2t_kwargs['encoder'] = encoderq2t\n",
    "\n",
    "agent_kwargs=dict(\n",
    "    name='SACAgentGraph',\n",
    "    policy_constructor=GaussPolicy,\n",
    "    qfunc_constructor=Qfunc,\n",
    "    env_constructor=GraphEnv,\n",
    "    buffer_constructor=GraphBuffer,\n",
    "    optimiser_constructors=dict(\n",
    "        policy_optim=torch.optim.Adam,\n",
    "        temperature_optim=torch.optim.Adam,\n",
    "        Q1_optim=torch.optim.Adam,\n",
    "        Q2_optim=torch.optim.Adam,\n",
    "    ),\n",
    "    entropy_lb=encoder_hiddens[-1],\n",
    "    policy_lr=1e-3,\n",
    "    temperature_lr=1e-3,\n",
    "    qfunc_lr=1e-3,\n",
    "    tau=0.005,\n",
    "    discount=1.,\n",
    "    save_to=TEST_OUTPUTS_PATH,\n",
    "    cache_best_policy=False,\n",
    "    clip_grads=False,\n",
    "    UT_trick=False,\n",
    "    with_entropy=False,\n",
    ")\n",
    "\n",
    "config = dict(\n",
    "    training_kwargs=dict(\n",
    "        seed=0,\n",
    "        num_iters=50,\n",
    "        num_steps_to_sample=200,\n",
    "        num_grad_steps=1,\n",
    "        batch_size=100,\n",
    "        num_eval_steps_to_sample=n_nodes,\n",
    "        min_steps_to_presample=100,\n",
    "    ),\n",
    "    Q1_kwargs=Q1_kwargs,\n",
    "    Q2_kwargs=Q2_kwargs,\n",
    "    Q1t_kwargs=Q1t_kwargs,\n",
    "    Q2t_kwargs=Q2t_kwargs,\n",
    "    policy_kwargs=gauss_policy_kwargs,\n",
    "    buffer_kwargs=dict(\n",
    "        max_size=10_000,\n",
    "        nodes=nodes,\n",
    "        seed=0,\n",
    "        drop_repeats_or_self_loops=True,\n",
    "        get_batch_reward=True,\n",
    "        graphs_per_batch=100,\n",
    "        action_is_index=True,\n",
    "        per_decision_imp_sample=False,\n",
    "    ),\n",
    "    env_kwargs=dict(\n",
    "        x=nodes,\n",
    "        reward_fn=reward_fn,\n",
    "        max_episode_steps=n_nodes,\n",
    "        num_expert_steps=n_nodes,\n",
    "        max_repeats=n_nodes // 3,\n",
    "        max_self_loops=n_nodes // 3,\n",
    "        drop_repeats_or_self_loops=True,\n",
    "        id=None,\n",
    "        reward_fn_termination=False,\n",
    "        calculate_reward=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "agent = SACAgentGraph(\n",
    "    **agent_kwargs,\n",
    "    **config\n",
    ")\n",
    "\n",
    "# print(agent.buffer.idx, agent.buffer.reward_idx)\n",
    "# agent.buffer.collect_path(agent.env, agent, agent.num_steps_to_sample)\n",
    "# print(agent.buffer.idx, agent.buffer.reward_idx, \n",
    "#       np.mean(agent.buffer.path_lens), \n",
    "#       np.max(agent.buffer.path_lens), \n",
    "#       agent.buffer.path_lens,\n",
    "#       agent.buffer.reward_t[:agent.buffer.reward_idx],\n",
    "#       sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-6.6551, grad_fn=<AddBackward0>), tensor(3.0458e+19), 0, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.buffer.get_single_ep_rewards_and_weights(agent.env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.buffer.idx, agent.buffer.reward_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_irl.irl_trainer import IRLGraphTrainer\n",
    "\n",
    "a = [0] + torch.repeat_interleave(torch.arange(1, n_nodes), 2).tolist() + [0]\n",
    "\n",
    "expert_edge_index = torch.tensor([\n",
    "    a,\n",
    "    ((torch.tensor([1, -1] * (len(a) // 2)) + torch.tensor(a)) % n_nodes).tolist()\n",
    "], dtype=torch.long)\n",
    "\n",
    "irl_trainer = IRLGraphTrainer(\n",
    "    reward_fn=reward_fn,\n",
    "    reward_optim=torch.optim.Adam(reward_fn.parameters(), lr=1e-3),\n",
    "    agent=agent,\n",
    "    nodes=nodes,\n",
    "    expert_edge_index=expert_edge_index,\n",
    "    num_expert_traj=5, \n",
    "    num_generated_traj=5,\n",
    "    num_graphs_in_batch=config['buffer_kwargs']['graphs_per_batch'], \n",
    "    reward_optim_lr_scheduler=None,\n",
    "    reward_grad_clip=False,\n",
    "    per_decision_imp_sample=config['buffer_kwargs']['per_decision_imp_sample'],\n",
    "    match_expert_step_count=True,\n",
    "    add_expert_to_generated=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:27<00:00,  1.83it/s]\n",
      "100%|██████████| 1/1 [00:27<00:00, 27.46s/it]\n"
     ]
    }
   ],
   "source": [
    "irl_trainer.train_policy_k_epochs(k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-11.5431, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irl_trainer.get_avg_generated_returns()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env-conda-py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
